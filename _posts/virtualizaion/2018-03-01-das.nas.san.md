---
toc: true
toc_label: "DAS, NAS, SAN 简介"
toc_icon: "code"
title: "DAS, NAS, SAN 简介"
tags: das nas san iscsi
categories: "storage"
classes: wide
excerpt: "常见存储方案"
header:
  overlay_image: /assets/images/header/programming.jpg
  overlay_filter: rgba(0, 0, 0, 0.6)
---




所谓的存储方案，就是用单独的软硬件将磁盘/磁盘组管理起来，供主机使用。

根据服务器类型分为：封闭系统的存储和开放系统的存储，封闭系统主要指大型机，开放系统指基于 Windows、UNIX、Linux 等操作系统的服务器；开放系统的存储分为：内置存储和外挂存储。

存储方案内部使用的硬盘，多为 SATA/SAS，追求高性能时也用 SSD，经过串联或 RAID 之后，对主机提供访问接口。

![image-center](/assets/images/storage.cates.png){: .align-center}

![image-center](/assets/images/das.nas.san.png){: .align-center}

SAS：Serial Attached SCSI


## DAS

Direct Attached Storage

直连存储，是指将存储设备通过总线（SCSI、PCI、IDE 等）接口直接连接到一台计算机上。

DAS 购置成本低，配置简单，因此对于小型企业很有吸引力。在中小企业中，许多数据应用必须安装在直连的 DAS 存储器上。

![image-center](/assets/images/storage-das.png){: .align-center}

采用 DAS 的服务器结构，其外部数据存储设备采用 SCSI 或 FC（Fibre Channel）技术，直接挂接在 **内部总线** 上的方式，数据存储是整个服务器结构的一部分。

DAS 这种直连方式，能够解决单台服务器的存储空间扩展、高性能传输需求，并且单台外置存储系统的容量，已经从不到 1TB，发展到了 2TB，随着大容量硬盘的推出，单台外置存储系统容量还会上升。此外，DAS 还可以构成基于 RAID 的双机高可用系统，满足数据存储对高可用的要求。从趋势上看，作为一种存储模式，DAS 仍然会继续得到应用。

DAS 不算是网络存储，因为 **只能被挂载它的主机访问**。因此，服务器发生故障时，连接在服务器上的 DAS 存储设备中的数据暂时不能被存取。





#### 适用场景

* 服务器在地理分布上很分散，很难用 SAN 或 NAS 互连，如商店或银行的分支。
* 存储系统必须直连到服务器，如数据库应用、邮件服务等。





#### 缺点


##### 占用主机资源

DAS **依赖** 主机的 **操作系统** 进行数据的 IO 读写和存储维护管理。

数据备份操作复杂。

在数据备份和恢复过程中，数据需要先回流到主机，再传输到直连的磁带机上。会占用较多的主机资源，约为 20-30%。因此许多企业用户的日常数据备份常常在深夜或业务系统不繁忙时进行，以免影响正常业务系统的运行。直连式存储的数据量越大，备份和恢复的时间就越长，对服务器硬件的依赖性和影响就越大。

##### 扩展性差

服务器本身容易成为系统瓶颈。

DAS 与主机通常采用 **SAS** 连接。随着 CPU 的处理能力越来越强，硬盘空间越来越大，阵列的硬盘数量越来越多，SAS 通道将会成为 IO 的瓶颈；主机的 **SCSI ID 资源有限**，能够建立的 SAS 通道连接有限。

##### 不支持高可用

服务器发生故障，数据不可访问。DAS 在扩展存储阵列容量时，会造成业务系统的停机，从而给企业带来经济损失。对于银行、电信、传媒等行业7×24小时服务的关键业务系统，这是不可接受的。并且直连式存储或服务器主机的升级扩展，只能由原设备厂商提供，往往受原设备厂商限制。

##### 无法动态分配存储空间

对于存在多个服务器的系统来说，设备分散，不便管理。同时多台服务器使用 DAS 时，存储空间不能在服务器之间动态分配，可能造成相当的资源浪费。



















## NAS

Network Attached Storage

网络连接存储，采用网络（TCP/IP、ATM、FDDI）技术，通过网络交换机连接存储系统和服务器主机，建立专用于数据存储的存储私网。

NAS 被定义为一种特殊的专用数据存储服务器，包括存储设备（磁盘阵列、光驱、磁带驱动器、便携存储介质等）和内置的系统软件，可提供 **跨平台文件共享** 功能。

NAS 通常在一个 LAN 上占有自己的节点，无需应用服务器的干预，允许用户在网络上存取数据，在这种配置中，NAS 集中管理和处理网络上的所有数据，将负载从应用或企业服务器上分离开来，有效降低总拥有成本，保护用户投资。

NAS 本身能够支持多种协议（如 NFS、CIFS、FTP、HTTP 等），而且能够支持各种操作系统。通过任何一台工作站，用浏览器就可以对 NAS 设备进行直观方便的管理。

NAS 是一种专用数据存储服务器。它以数据为中心，将存储设备与服务器彻底分离，集中管理数据，从而释放带宽、提高性能、降低总拥有成本、保护投资。其成本远远低于使用服务器存储，而效率却远远高于后者。目前国际著名的 NAS 企业有 Netapp、EMC、OUO 等。

NAS 是 **文件级** 的存储方法，它的重点在于帮助工作组和部门级机构解决迅速增加存储容量的需求。NAS 主要用于文件共享，例如需要共享大型 CAD 文档的工程小组。

NAS 是 **功能单一的精简型电脑**。NAS 在架构上与个人电脑相似，但因功能单纯，可移除许多不必要的连接器、控制晶片、电子回路，如键盘、鼠标、USB、VGA等。

![image-center](/assets/images/NAS-Diagram-932x523.gif){: .align-center}





#### 优点

##### 即插即用

NAS 产品是真正 **即插即用** 的产品。NAS 设备一般支持 **多计算机平台**，用户通过网络支持协议可进入相同的文档，因而 NAS 设备无需改造即可用于混合 Unix/Windows 局域网内。

##### 不受物理位置限制

NAS 设备的物理位置可以很灵活，可放置在工作组内，靠近数据中心的应用服务器，也可放在其他地点，通过物理链路与网络连接起来。

##### 不占用主机资源

NAS 设备的目的就是为了 **分离** 网络设备中的服务器和存储。这样做能够让服务器（特别是通用服务器）有更多的计算资源来处理用户的各种应用和业务（如电子邮件处理、远程应用等）。NAS 则用来帮助服务器完成一些文件的任务和 I/O 的操作。

##### 高可扩展性

NAS 的扩展只需通过添加一个节点及网络设备即可，做到了真正的即插即用，并且部署位置非常灵活。基本上启动 NAS 设备，运行相应的网络文件系统，并将这个 NAS 设备接入网络环境就完成了添加。高级 NAS 更能做到支持网络接口的热安装、硬盘的的随时增加等。

当前 NAS 的扩展趋势已经演变成为 **NAS 集群** 技术了。NAS 集群技术通过一组 NAS 设备集合来形成如同一个 NAS 设备，NAS 集群技术提供了一定的 **存储分流**，使得不同的 NAS 设备可以同时工作，以满足网络存储需求，从而提高了整个 NAS 系统的整体性能，并解决了多个 NAS 系统的扩展性和管理趋于复杂性的问题。

##### 易于管理

NAS 本身就是为了企业内部网络而设计，实现了异构平台下的数据共享，因此 NAS 的使用和维护成本就相对很低，管理和维护工作也相对简单。用户只需一些简单的初期设置和管理，NAS 设备就可以很好的运行起来。





#### 缺点

传统的 TCP/IP 协议不可避免的给 NAS 带来一些 “先天” 的缺点。

##### 存储性能的局限

NAS 虽然比传统的 DAS 设备在存储性能上有很大的提高，但是只适合较小的网络或局域网内。在 SAN 中，已经将备份数据流从 LAN 中转移了出去，而 NAS 却仍在使用网络进行备份和恢复。它将存储事务由并行 SCSI 连接转移到了网络上。这就是说，LAN 除了必须处理正常的最终用户传输流外，还必须处理包括备份操作的存储磁盘请求。

由于存储数据通过普通数据网络传输，因此易受网络上其它流量的影响。当网络上有其它大数据流量时会严重影响系统性能。当多台客户端访问 NAS 文件系统时，性能会大大的下降，最终不能满足用户的需求。

##### 数据安全

由于存储数据通过普通数据网络传输，因此容易产生数据泄漏等安全问题。

##### 可靠性有待提高

当企业内部网络发展到一定的规模时，NAS 设备的数据服务和数据管理形成了网络的双重负担，也就是说 NAS 除了要处理正常的终端数据 I/O 请求外，还需要做备份和恢复等操作。并且 NAS 后期的扩容成本高；一般的 NAS 没有高可用配置，容易形成单点故障。





















## SAN

Storage Area Network

存储区域网络，是一种连接外接存储设备和服务器的架构。采用光纤通道技术、磁盘阵列、磁带柜、光盘柜等来实现。连接到服务器的存储设备，将被操作系统 **视为直接连接的存储设备**。

SAN 通常是一个存储专用的 **独立的网络**，其它设备无法通过 LAN 来访问。SAN 有时被称为 **服务器后面的网络**。这个专用的高速网络中，所有的存储设备都是互联的。在存储设备之间发生的数据传输，如备份数据流，是发生在服务器后面的，对于服务器来说是不可见的。

SAN 专门开发了新的协议，如光纤通道 FC。因此 SAN 有其专用的网络设备和存储设备，都需要单独购买、安装、配置，使得 SAN 比 NAS 架构更加昂贵。

除针对大型企业的企业级存储方案外，2000 年之后，SAN 的价格和复杂度有所下降，越来越多的中小型企业也在逐步采用该项技术。

SAN 不提供针对文件的服务，它进行的是 **基于块的操作**。

与 SAN 相比较，NAS 设备使用的是基于文件的通信协议，如 NFS、SMB、CIFS 等通信协议，它们被明确的定义为 **远程** 存储设备，计算机请求访问的是抽象文件的一段内容，而非对磁盘进行的块设备操作。

![image-center](/assets/images/san1.jpg){: .align-center}









#### SAN 的组成

光纤模块：GBIC，Gigabit Interface Converter。用于转换电流与光信号。

HBA 卡：Host Bust Adapter



##### 主机层

允许访问 SAN 及其存储设备的服务器组成了 SAN 的主机层。这些服务器中含有 HBA，这是一种专用的硬件，插在主板的插槽中，有其对应的固件及驱动。服务器上的操作系统通过 HBA 可以与 SAN 中的存储设备进行通信。

双端口 8Gb 光纤 HBA 卡：

![image-center](/assets/images/FC_HBA.jpg){: .align-center}

光纤通过光纤模块连接到 HBA 上，在光纤交换机与存储设备上也需要光纤模块。光纤模块会把数字信号转换为光脉冲，然后才能在存储网络中传输。在另一端，再把光脉冲转换为数字信号。



##### 结构层

SAN 中的 **网络设备** 统称为结构层，其中包括 SAN 交换机、路由器、网桥、网关设备、线缆。

SAN 网络设备在存储网络中传输数据时，其中的一端称为 **initiator**，发起端，如服务器的 HBA 端口；另一端称为 **target**，目标，如存储设备的端口。

###### SAN 交换机

![image-center](/assets/images/san.switches.jpg){: .align-center}

SAN 网络通常有冗余，因此 SAN 交换机都有冗余连接。SAN 交换机在把服务器与存储设备连接起来时，通常是非阻塞的，因此可以同时在多条线缆中同时传输数据。早期的 SAN 中，集线器是唯一支持光纤通道的设备，而今产生了光纤通道交换机，很少再使用 HUB 了。交换机的优势是允许所有连接设备同时传输，因为它为每一对传输端口都提供专用的连接。

出于冗余考虑，SAN 交换机会采用网状拓扑。一台 SAN 交换机可以有 8 ~ 32 个端口，director 级别的交换机则可以有 128 个端口。

>网状拓扑：mesh topology。网络中的每个节点都彼此互联，用来实现最大可能的传输，即使其中的一个连接断开也不会影响正常传输。

###### 线缆

早期的 SAN 只能使用铜缆，如今大多使用多模光纤。光纤通道使用的是 FC-SW-6 协议，该协议规定 SAN 中的每个设备都要在 HBA 中内置 WWN 地址，即 World Wide Name 地址。某个设备连接到 SAN 以后，其 WWN 会在 SAN 交换机的域名服务器中注册。SAN 光纤通道存储设备厂商也可以不内置 WWN，而改为内置 WWNN，即 World Wide Node Name。存储设备的端口其 WWN 通常以 `5` 开头，而服务器的 HBA 卡则以 `10` 或 `21` 开头。



##### 存储层

在光纤通道交换协议的上一层通常是 SCSI 协议，部署于服务器与 SAN 存储设备之间。通过它，应用程序可以进行通信，或对数据进行编码，以供存储设备使用。

虽然 SAN 中也可以使用 iSCSI 和 Infiniband 协议，但它们通常是桥接到光纤通道中。然而，仍然可以使用 Infiniband 和 iSCSI 的存储设备，尤其是磁盘阵列。

SAN 中的各种存储设备构成了存储层，包含各种硬盘和磁带设备。在 SAN 中，多个磁盘阵列通过 RAID 连接在一起，使得多个磁盘可以形成一个巨大的逻辑磁盘。

###### LUN

Logical Unit Number。SAN 中，存储设备的地址，通常用来标识一个逻辑磁盘。

每个存储设备，甚至其中的每个分区都会被分配一个 LUN。它是 SAN 中唯一的数字，每个节点（服务器或其它存储设备）都可以使用 LUN 来访问存储设备。

可以通过配置，让某个或某组服务器只能访问存储层中特定的部分，也是通过 LUN 来标识。

当一个存储设备收到读或写的请求时，它会检查自己的访问列表以确定该节点是否有权访问本存储区域。此时，发出请求的节点是用 LUN 来标识的，而被访问的存储区域也是用 LUN 标识的。

HBA 以及 SAN 的应用程序会使用 LUN 掩码技术来决定限制哪些服务器的访问。另一种进行访问控制的方法是使用基于光纤的访问控制，必须部署于 SAN 网络设备与服务器上，通过划分不同的地带（zone）来限制服务器对不同存储设备的访问。





#### 网络协议

大多数 SAN 使用 **SCSI 接口** 进行服务器与磁盘驱动设备之间的通信，该总线拓扑结构不适用于网络环境，所以没有使用底层物理连接介质（如连接电缆），而是采用其它底层通信协议作为 **映射层**（mapping layer） 来实现网络连接：

* FCP：Fibre Channel Protocol，光纤通道协议，是最常见的通过光纤通道来映射 SCSI 的一种连接方式；
* iSCSI：基于 TCP/IP 的 SCSI 映射；
* HyperSCSI：基于以太网的 SCSI 映射；
* AoE：ATA over Ethernet，基于以太网的 ATA 映射；
* FICON：FIber Connector，光纤连接器，大型主机通道，以光纤通道标准为基础；
* FCoE：Fibre Channel over Ethernet，基于以太网的 FC 协议；
* iSER：iSCSI Extensions for RDMA，基于 InfiniBand（IB）的 iSCSI 连接；
* iFCP 或 SANoIP：基于 IP 网络的光纤通道协议。

另外，也可以用 SAS 和 SATA 技术来组建存储网络。SAS 是由 SCSI 直连存储进化而来，SATA 是由 IDE 直连存储进化而来。SAS 和 SATA 设备可以使用 SAS 扩展卡来组网。

使用 SCSI 的协议栈示例：

<table class="wikitable" style="text-align:center">
<tbody><tr>
<td colspan="7">Applications
</td></tr>
<tr>
<td colspan="7">SCSI Layer
</td></tr>
<tr>
<td rowspan="4">FCP
</td>
<td rowspan="3">FCP
</td>
<td>FCP
</td>
<td>FCP
</td>
<td rowspan="2">iSCSI
</td>
<td rowspan="2">iSER
</td>
<td rowspan="2">SRP
</td></tr>
<tr>
<td>FCIP
</td>
<td>iFCP
</td></tr>
<tr>
<td colspan="3">TCP
</td>
<td colspan="2">RDMA Transport
</td></tr>
<tr>
<td>FCoE
</td>
<td colspan="3">IP
</td>
<td colspan="2">InfiniBand Network
</td></tr>
<tr>
<td>FC
</td>
<td colspan="4">Ethernet
</td>
<td colspan="2">Ethernet or InfiniBand Link
</td></tr></tbody></table>





#### SAN 软件

SAN 不单单是由通信设施组成，还包括其软件管理层。SAN 软件用来管理服务器、存储设备和网络，以保证数据的正常传输和存储。SAN 中的 **存储设备并非由服务器专属和管理**，实际上，某个服务器所能够访问到的存储量是 **无限的**，而且该存储量也可以被其他服务器访问。另外，SAN 的软件必须确保数据在 SAN 中是在存储设备之间直接移动的，必须保证受到服务器最小程度的干扰。

SAN 的管理软件安装于一个或多个服务器上，管理客户端则安装在存储设备上。

SAN 管理软件有两种部署方法：

* 带内管理：in-band management，服务器与存储设备之间所传输的管理数据，与存储数据 **共用同一个网络**。
* 带外管理：out-of-band management，**管理数据使用专用网络**。

SAN 管理软件会从存储层的所有存储设备中收集管理数据，包括读写错误、存储容量瓶颈以及存储设备失效等信息。SAN 管理软件可以集成 SNMP 协议。

1999 年，引入了一种用于管理存储设备并提供互通性的开放标准，即 CIM，Common Information Model，通用信息模型。基于 WEB 的 CIM 称为 WBEM，Web-Based Enterprise Management，它定义了 SAN 存储设备对象以及进程事务（process transactions）。这些协议的使用还涉及到一个 CMI 对象管理器，用于管理对象及其操作，并可实现 SAN 存储设备的集中管理。基本的设备管理可以使用 SMI-S，Storage Management Interface Specification，其中，CMI 对象和进程会在目录中注册，然后就可以在该目录中规划应用程序及子系统。管理软件还可以配置存储设备，例如可以配置存储设备的 zone 和 LUN。

每一个 SAN 供应商都有自家的管理和配置系统，要想在 SAN 中实现统一管理多家厂商的设备，还要依赖于厂商开放其 API，只有这样，才有可能用最顶层的软件来统一管理所有厂家的设备。






#### SAN 文件系统

在 SAN 中，数据是以块的级别进行传输、存储和读取的，因此，SAN 无法提供数据文件的提取，只能提供块级别的存储和操作。不过已经开发出了新的文件系统，称为 SAN 文件系统或共享磁盘文件系统，可以配合 SAN 软件一同使用，以提供文件级的访问服务。服务器的各个文件系统会在其专属的、非共享的 LUN 上保有其自己的文件系统，就好像这些文件系统是本地专属的一样。如果让多个操作系统简单地来共享一个 LUN 的话，会彼此发生干扰，很快会损坏数据。要想在不同计算机之间共享同一个 LUN 的话，还是需要 SAN 文件系统或计算机集群来实现。





#### 存储共享

出于历史原因，数据中心中最初都是 SCSI 磁盘阵列的 “孤岛” 群。每个单独的小 “岛屿” 都是一个专门的直连存储器，被视作无数个 “虚拟硬盘驱动器”（如 LUNs）。

本质上来说，SAN 就是将一个个存储 “孤岛” 使用高速网络连接起来，这样使得所有的应用可以访问所有的磁盘。
{: .notice}

操作系统会将 SAN 视为一组 LUN，并且在 LUN 上维护自己的文件系统。这些本地文件系统，不能在多个操作系统/主机之间进行共享，具有非常高的可靠性和十分广泛的应用。如果两个独立的本地文件系统存在于一个共享的 LUN 上，没有任何机制能让它们彼此了解对方的存在，没有类似缓存同步的机制，所以可能发生数据丢失的情况。因此，在主机之间通过 SAN 共享数据，需要一些复杂的高级解决方案，例如 SAN 文件系统或者计算机集群。撇开这些问题，SAN 对于提高存储能力的应用有很大帮助，因为多个服务器可以共享磁盘阵列上的存储空间。

SAN 的一项典型应用是需要高速块级别访问的数据操作服务器，比如电子邮件服务器、数据库、高利用率的文件服务器等。

相对地，NAS 允许多台计算机经过网络访问同一个文件系统，并且会自动同步它们的操作。由于 NAS head 的引入，使得 SAN 存储可以很容易地转换为 NAS。





#### 存储虚拟化

存储虚拟化是指从物理存储抽象化为逻辑存储的过程。把所有物理存储资源聚合到 **存储池** 中，在这里创建出 **逻辑存储**。最后呈现给用户的，是一个用来存储数据的逻辑空间，在用户看不到的背后，是将逻辑存储映射到物理存储的过程，这个概念称之为 location transparency。该机制广泛使用在当下的磁盘阵列产品上，使用的是厂商私有技术。然而，存储虚拟化的最终目标是把散布在网络中的、不同厂商的多个磁盘阵列集中在一起使用，将它们变成一个单一的存储设备，然后这个单一的存储设备才能统一管理。





#### 优势

##### 易于管理和维护

存储器的共享通常简化了存储器的维护，提高了管理的灵活性，因为连接电缆和存储器设备不需要物理地从一台服务器上搬到另外一台服务器上。

##### 可引导

可以从 SAN 来启动并引导服务器的操作系统。

##### 高可用

SAN 可被重新配置，在服务器出现故障时，更换会变得简单、快速。更换后的服务器可以继续使用先前故障服务器的 LUN。这个更替服务器的过程可以被压缩到 **半小时** 之短。Brocade 的应用资源管理器 Application Resource Manager，可以自动管理那些能够从 SAN 启动的服务器，而完成操作的时间通常只需 **几分钟**。尽管此方向的技术现在仍然很新，还在不断演进，许多人认为它将进入未来的企业级数据中心。

##### 灾难恢复

SAN 也被设计为可以提供更有效的灾难恢复。一个 SAN 可以 “携带” 距离相对较远的第二个存储阵列。这就使得存储备份可以使用多种实现方式，可能是磁盘阵列控制器、服务器软件或其它 SAN 设备。因为 IP 广域网通常是最经济的长距离传输方式，所以基于 IP 的光纤通道和基于 IP 的 ISCSI 协议，就成为了通过 IP 网络扩充 SAN 的最佳方式。使用传统的物理 SCSI 层连接的 SAN 仅仅可以提供数米的连接距离，完全无法满足灾难恢复的不间断业务的需求。

磁盘阵列的，加速了许多功能的发展，包括I/O缓存、存储快照、卷克隆（Business_Continuance_Volumes, BCV）等。
